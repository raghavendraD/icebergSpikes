// start a shell
spark-shell --packages org.apache.iceberg:iceberg-spark3-runtime:0.11.0 --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions --conf spark.sql.catalog.spark_catalog=org.apache.iceberg.spark.SparkSessionCatalog --conf spark.sql.catalog.spark_catalog.type=hive --conf spark.sql.catalog.local=org.apache.iceberg.spark.SparkCatalog --conf spark.sql.catalog.local.type=hadoop --conf spark.sql.catalog.local.warehouse=$PWD/warehouse

case class Person (id: Int, name: String, date : String )
val r = 1 to 100000 toList
val df = spark.createDataset(r.map(i => Person(i, "Mohit"+i, "2021-06-0"+(i%9+1 ))))
spark.sql("create table local.db.person (id int, name string, date string) USING iceberg")
df.writeTo("local.db.person").create()

// start another shell
spark-shell --packages org.apache.iceberg:iceberg-spark3-runtime:0.11.0 
--conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions 

import org.apache.spark.sql.{Row, SparkSession}
import org.apache.iceberg.{PartitionSpec, Schema, Table}
import org.apache.iceberg.catalog.{Namespace, TableIdentifier}
import org.apache.iceberg.hadoop.{ConfigProperties}
import org.apache.iceberg.hadoop.{HadoopCatalog}
import org.apache.iceberg.types.{Type, Types}
import org.apache.hadoop.conf.{Configuration}
import org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType};
import org.apache.iceberg.expressions.Expressions.{and, alwaysTrue, equal};
import org.apache.iceberg.types.{Type, Types}    
import org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType}
import java.util.{Calendar, Date}
import org.apache.spark.sql.types._
import org.apache.spark.sql
import org.apache.spark.sql.SparkSession
import spark.implicits._
import org.apache.iceberg.actions.{Actions}
import org.apache.iceberg.expressions.Expressions

val warehouse = "/Users/rdesai/warehouse";
val hadoopConfig = new Configuration();
hadoopConfig.set("spark.sql.catalog.local", "org.apache.iceberg.spark.SparkCatalog")
hadoopConfig.set("spark.sql.catalog.local.type", "hadoop")
hadoopConfig.set("spark.sql.catalog.local.warehouse", warehouse)
val catalog = new HadoopCatalog(spark.sessionState.newHadoopConf(), "file://" + warehouse)

val name = TableIdentifier.of("db","person");
val table = catalog.loadTable(name);

// compaction of data and manifest files
Actions.forTable(table).rewriteDataFiles().targetSizeInBytes(500 * 1024 * 1024).execute()
table.rewriteManifests().clusterBy(x => "x").commit();

// compaction of a specific column/partition/data
import org.apache.iceberg.expressions.Expressions
Actions.forTable(table).rewriteDataFiles().filter(Expressions.equal("person", "mohit1")).targetSizeInBytes(500 * 1024 * 1024).execute()

// expire snapshots
spark.sql("CREATE TABLE local.db.person (id bigint, name string, state string) USING iceberg")

val r = 1 to 100 toList
r.foreach( i => { spark.sql(s"insert into local.db.person values ($i, 'Mohit', '2021-06-$i') ") })

spark.sql(s" CALL local.system.expire_snapshots(table => 'db.person', older_than => TIMESTAMP '2021-06-24 01:08:00.00', retain_last => 2 ) ").show
+------------------------+----------------------------+----------------------------+
|deleted_data_files_count|deleted_manifest_files_count|deleted_manifest_lists_count|
+------------------------+----------------------------+----------------------------+
|                       0|                           0|                          98|
+------------------------+----------------------------+----------------------------+

// clean up orphan files
spark.sql(s" CALL local.system.remove_orphan_files(table => 'db.person2', dry_run => true) ").show
